{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation metrics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7F4QhR7xoRN"
      },
      "source": [
        "# Evaluation Matrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfHcCZ5MxdkX"
      },
      "source": [
        "1. Confusion Martix\n",
        "2. False positive rate | Type-I error\n",
        "3. False negative rate | Type-II error\n",
        "4. True negative rate | Specificity\n",
        "5. Negative predictive value\n",
        "6. False discovery rate\n",
        "7. True positive rate | Recall | Sensitivity\n",
        "8. Positive predictive value | Precision\n",
        "9. Accuracy\n",
        "10. F beta score\n",
        "11. F1 score\n",
        "12. F2 score\n",
        "13. Cohen Kappa\n",
        "14. Matthews correlation coefficient\n",
        "15. ROC curve\n",
        "16. ROC AUC score\n",
        "17. Precision-Recall curve\n",
        "18. Corcondance And Discordance Ratio\n",
        "19. Somars D\n",
        "20. kendall's Tau \n",
        "21. Log loss\n",
        "22. Brier score\n",
        "23. Gini Coefficient\n",
        "24. Kolmogorov-Smirnov plot And Kolmogorov Smirnov statistics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Das4OuYaPsSQ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It_PQHWJU7L6"
      },
      "source": [
        "# 1. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ignDgaRZxhmC"
      },
      "source": [
        "y_pred=[]\n",
        "def cm(y_test,y_pred_pos,threshold):\n",
        "  for i in y_pred_pos:\n",
        "    if i > threshold:\n",
        "      y_pred.append(1)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "  y_pred\n",
        "  sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYRdwTJlohKz"
      },
      "source": [
        "Interpretation : \n",
        "\n",
        "It compares actual values and predicted values.\n",
        "\n",
        "1. The diagonal elements represents the correct predictions.\n",
        "\n",
        "2. Where other elements are false positive and false negative i.e. type 1  and type 2 error. which is we always try to reduce that values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt9Np1xyVHRy"
      },
      "source": [
        "# 2. Flase Positive Rate (Type 1 Error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D1giqN1U6Gt"
      },
      "source": [
        "def fpr(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  fpr=fp/(fp+tn)\n",
        "  print(\"False Positive Rate is\",fpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMGVv8gwtySM"
      },
      "source": [
        "Interpretation :\n",
        "\n",
        "It should be low or 0. A model with low fpr is always better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAgV5Xg3esnT"
      },
      "source": [
        "# 3. False Neagtive Rate (Type 2 Error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnP9FUmweq_F"
      },
      "source": [
        "def fnr(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  fnr=fn/(fn+tp)\n",
        "  print(\"False Negative Rate is\",fnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e-al3e4ujac"
      },
      "source": [
        "Interpretation :\n",
        "\n",
        "It should be low or 0. A model with low fnr is always better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGJoCuCafmpT"
      },
      "source": [
        "#  4. True Negative Rate | Specificity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRBvT6ZjfWv1"
      },
      "source": [
        "def tnr(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  tnr=tn/(tn+fp)\n",
        "  print(\"True Negative Rate is\",tnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB44q5fpgIxM"
      },
      "source": [
        "# 5. Negative Predictive Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koS0gfuNfWyz"
      },
      "source": [
        "def npv(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  npv=tn/(tn+fn)\n",
        "  print(\"Negative Predicted Value is\",npv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmrXQovbgjbb"
      },
      "source": [
        "# 6. False Discovery Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mepUG7XIfW1k"
      },
      "source": [
        "def fdr(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  fdr=fp/(fp+tp)\n",
        "  print(\"Flase Discovery Rate is\",fdr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_jyvFNHg45u"
      },
      "source": [
        "# 7.  True Positive Rate | Recall | Sensitivity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0s-wq_Vgi0B"
      },
      "source": [
        "def recall(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  tpr=tp/(tp+fn)\n",
        "  print(\"Recall is\",tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwcOw6FsSUU"
      },
      "source": [
        "### Interpretation:\n",
        "\n",
        "High recall relates to the low false negative rate. So high recall is better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0o3LliVhPtM"
      },
      "source": [
        "# 8. Positive Predictive Value | Precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7zOpmCtgi3g"
      },
      "source": [
        "def precision(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  ppv=tp/(tp+fp)\n",
        "  print(\"Precision is\",ppv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb1XCCs6r-8r"
      },
      "source": [
        "### Interpretation:\n",
        "\n",
        "\n",
        "The ideal value of the PPV, with a perfect test, is 1 (100%),and the worst possible value would be zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlQtP4SIhjid"
      },
      "source": [
        "# 9. Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_uqKgH3gi9v"
      },
      "source": [
        "def accuracy(y_test,y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  acc=(tp+tn)/(tp+fn+fp+tn)\n",
        "  print(\"Accuracy is\",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCromV8Nrlvg"
      },
      "source": [
        "Interpretation:\n",
        "\n",
        "If we have high accuracy then our model is best. Yes, accuracy is a great measure but only when we have symmetric datasets or balanced dataset, where values of false positive and false negatives are almost same. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtaacplwiJ3H"
      },
      "source": [
        "# 10. F beta score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmM4AvfCiWqs"
      },
      "source": [
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def f_beta(y_test,y_pred):\n",
        "  fbeta=fbeta_score(y_test,y_pred,beta)            ##beta=0.5 when fp more imp\n",
        "  print(\"f beta score is\",fbeta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm1ETUDY64_s"
      },
      "source": [
        "Interpretation :\n",
        "\n",
        "F(0.5)-score is used when False Positives is crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM9oUGFP4cmS"
      },
      "source": [
        "# 11. F1 score (beta=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URGNnztgiWtp"
      },
      "source": [
        "###when fp and fn equally important\n",
        "\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def f_one(y_test,y_pred):\n",
        "  fone=fbeta_score(y_test,y_pred,1)           \n",
        "  print(\"f beta score is\",fone)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iayWq-gH6xTl"
      },
      "source": [
        "Interpretation :\n",
        "\n",
        "F1-score is used when the False Negatives and False Positives are crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzzDHEcg40MP"
      },
      "source": [
        "# 12. F2 score (beta=2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2gK1TBEiWxP"
      },
      "source": [
        "### fn more important\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "def f_two(y_test,y_pred):\n",
        "  ftwo=fbeta_score(y_test,y_pred,2)            \n",
        "  print(\"f beta score is\",ftwo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8AtElXH6Wq1"
      },
      "source": [
        "Interpretation :\n",
        "\n",
        "F2 score (beta=2) It's a metric that combines precision and recall, putting 2x emphasis on recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVIzp5qb5UXP"
      },
      "source": [
        "# 13. Cohen Kappa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXZvktD87A0w"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def cohen_kappa(y_test,y_pred):\n",
        "  kappa=cohen_kappa_score(y_test,y_pred)\n",
        "  print(\"Cohen kappa score is\",kappa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKsaoSMocdBt"
      },
      "source": [
        "### **Interpretation**:\n",
        "\n",
        "Cohen’s kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless. There is no standardized way to interpret its values. values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreemen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VluMHn8z5lu-"
      },
      "source": [
        "# 14. Matthews correlation coefficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2-1TooZ7Bp_"
      },
      "source": [
        "def mcc(y_test,y_pred):\n",
        "  mcc=matthews_corrcoef(y_test,y_pred)\n",
        "  print(\"Matthews correlation coefficient is\",mcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiMzTzr4oq9y"
      },
      "source": [
        "**Interpretation : **\n",
        "\n",
        "MCC lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB9KHPzK5mEn"
      },
      "source": [
        "# 15. ROC AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQSN82wi7CYq"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def ra_score(y_test,y_pred):\n",
        "  ra_score=roc_auc_score(y_test,y_pred)\n",
        "  print(\"AUC_ROC Score is\",ra_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmy8E5dK5l61"
      },
      "source": [
        "# 16. ROC curve (AUC / C statistics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFORObNB7D94"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from numpy import trapz\n",
        "\n",
        "#### Get fpr and tpr at different threshold\n",
        "def Fpr_Tpr(y,y_pred_prob,threshold):\n",
        "    true_positive=0;\n",
        "    false_positive=0;\n",
        "    true_negative=0;\n",
        "    false_negative=0;\n",
        "    for i in range(len(y)):\n",
        "        if y[i]==1:\n",
        "            if y_pred_prob[i]>=threshold:\n",
        "                true_positive+=1\n",
        "            else:\n",
        "                false_negative+=1\n",
        "        else:\n",
        "            if y_pred_prob[i]>=threshold:\n",
        "                false_positive+=1\n",
        "            else:\n",
        "                true_negative+=1\n",
        "    tpr=true_positive/(true_positive + false_negative)\n",
        "    fpr=false_positive/(false_positive + true_negative)\n",
        "    return tpr,fpr\n",
        "\n",
        "\n",
        "#### Plot AUC-ROC\n",
        "def Curve(y,y_pred_prob):\n",
        "    TPR=[]\n",
        "    FPR=[]\n",
        "    THRESHOLD=[]\n",
        "    i=0\n",
        "    #increemental step size for threshold\n",
        "    dx_step=0.02;\n",
        "    while(i<=1):\n",
        "        threshold=i\n",
        "        tpr,fpr=Fpr_Tpr(y,y_pred_prob,threshold)\n",
        "        TPR.append(tpr)\n",
        "        FPR.append(fpr)\n",
        "        THRESHOLD.append(threshold)\n",
        "        i+=dx_step;\n",
        "        \n",
        "    plt.plot(FPR,TPR);\n",
        "    plt.plot(THRESHOLD,THRESHOLD,'--')\n",
        "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.show()\n",
        "    area = trapz(TPR, dx=dx_step)\n",
        "    print(\"AUC:Area under the ROC curve is\", area)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6tXsOhIpwPV"
      },
      "source": [
        "**Interpretation** :\n",
        "\n",
        "If C>=  0.9, the model is considered to have outstanding discrimination.\n",
        "Caution : The model may be faced with problem of over-fitting.\n",
        "\n",
        "If 0.8 <= C < 0.9, the model is considered to have excellent discrimination; \n",
        "\n",
        "If 0.7<=C <  0.8, the model is considered to have acceptable discrimination; \n",
        "\n",
        "If C = 0.5, the model has no discrimination (random case)\n",
        "\n",
        "If C < 0.5, the model is worse than random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w9vwnju5mON"
      },
      "source": [
        "# 17. Precision-Recall curve\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln5dDj107EhY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "### Get precision and recall at different threshold\n",
        "def precision_recall(y,y_pred_prob,threshold):\n",
        "    true_positive=0;\n",
        "    false_positive=0;\n",
        "    true_negative=0;\n",
        "    false_negative=0;\n",
        "    for i in range(len(y)):\n",
        "        if y[i]==1:\n",
        "            if y_pred_prob[i]>=threshold:\n",
        "                true_positive+=1\n",
        "            else:\n",
        "                false_negative+=1\n",
        "        else:\n",
        "            if y_pred_prob[i]>=threshold:\n",
        "                false_positive+=1\n",
        "            else:\n",
        "                true_negative+=1\n",
        "    precision=true_positive/(true_positive + false_positive)\n",
        "    recall=true_positive/(false_positive + false_negative)\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "### Plot Pecision Recall Curve\n",
        "def pr_curve(y,y_pred_prob):\n",
        "    pre=[]\n",
        "    rec=[]\n",
        "    THRESHOLD=[]\n",
        "    i=0\n",
        "    #increemental step size for threshold\n",
        "    dx_step=0.02;\n",
        "    while(i<=1):\n",
        "        threshold=i\n",
        "        precision,recall=precision_recall(y,y_pred_prob,threshold)\n",
        "        pre.append(precision)\n",
        "        rec.append(recall)\n",
        "        THRESHOLD.append(threshold)\n",
        "        i+=dx_step;\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10,10))    \n",
        "    plt.plot(pre,rec)\n",
        "    baseline=len(y[y==1])/len(y)\n",
        "    plt.plot([0,1],[baseline, baseline],linestyle='--',label='baseline')\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.show()\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vPvt3yskbSD"
      },
      "source": [
        "### Interpretation :\n",
        "\n",
        "There is no thumb rule regarding AUPRC which says whether model is good or bad. If you have 10% events, a random estimator would have a AUPRC of 0.1. You should evaluate several machine learning algorithms and then compare the AUPRC score and should see how far your AUPRC score is from random estimator score. It also depends on the context of the problem. Suppose you are building a model which identifies persons who are likely to be terrorist. It is a highly-imbalanced problem. In this case, AUPRC is more informative than AUC. It is important to note that a good ROC score doesn't necessarily mean a good Precision-Recall score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkpJFAiI6ASJ"
      },
      "source": [
        "# 18. Concordance and Discordance ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0RKJCAE7FKA"
      },
      "source": [
        "def corcondance(self):\n",
        "    df2=pd.DataFrame({'prob':self.prob,'resp':self.resp})\n",
        "    Event=df2.loc[df2.resp==1]\n",
        "    Non_event=df2.loc[df2.resp==0]\n",
        "    Pairs=0\n",
        "    Conc=0\n",
        "    Disc=0\n",
        "    Tie=0\n",
        "    for i in Event.prob:\n",
        "      for j in Non_event.prob:\n",
        "        Pairs+=1\n",
        "        if (i>j):\n",
        "          Conc+=1\n",
        "        elif (i<j):\n",
        "          Disc+=1\n",
        "        else:\n",
        "          Tie+=1\n",
        "    print(\"__________________________________________________________\")\n",
        "    print(\"Total pairs\",Pairs)\n",
        "    print(\"The percentage of Concordance\",round(Conc/Pairs*100,3),\"%\")\n",
        "    print(\"The percentage of Discordance\",round(Disc/Pairs*100,3),\"%\")\n",
        "    print(\"The percentage of Tie\",round(Tie/Pairs*100,3),\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQDjQrU-rKpM"
      },
      "source": [
        "**Interpretation :**\n",
        "\n",
        "1. A good classification model should have concordance above 80%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7H5W6YzDfBA"
      },
      "source": [
        "# 19. Somers D = (#Concordant Pairs - #Discordant Pairs - #Ties) / Total Pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYeVV8FMD6KA"
      },
      "source": [
        "def somers_d(self):\n",
        "    df2=pd.DataFrame({'prob':self.prob,'resp':self.resp})\n",
        "    Event=df2.loc[df2.resp==1]\n",
        "    Non_event=df2.loc[df2.resp==0]\n",
        "    Pairs=0\n",
        "    Conc=0\n",
        "    Disc=0\n",
        "    Tie=0\n",
        "    for i in Event.prob:\n",
        "      for j in Non_event.prob:\n",
        "        Pairs+=1\n",
        "        if (i>j):\n",
        "          Conc+=1\n",
        "        elif (i<j):\n",
        "          Disc+=1\n",
        "        else:\n",
        "          Tie+=1\n",
        "    print(\"__________________________________________________________\")\n",
        "    print(\"Somers D is\",(Conc-Disc-Tie)/Pairs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNcjy-DhsHHq"
      },
      "source": [
        "### Interpretation :\n",
        "\n",
        "Large values for Somers' D (tending towards -1 or 1) suggest the model has good predictive ability. Smaller values (tending towards zero in either direction) indicate the model is a poor predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIcmoSjFDmn6"
      },
      "source": [
        "## 20. Kendall’s Tau = (C — D / C + D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5pkUwwPFeGX"
      },
      "source": [
        "def kendallsTau(self):\n",
        "    df2=pd.DataFrame({'prob':self.prob,'resp':self.resp})\n",
        "    Event=df2.loc[df2.resp==1]\n",
        "    Non_event=df2.loc[df2.resp==0]\n",
        "    Pairs=0\n",
        "    Conc=0\n",
        "    Disc=0\n",
        "    Tie=0\n",
        "    for i in Event.prob:\n",
        "      for j in Non_event.prob:\n",
        "        Pairs+=1\n",
        "        if (i>j):\n",
        "          Conc+=1\n",
        "        elif (i<j):\n",
        "          Disc+=1\n",
        "        else:\n",
        "          Tie+=1\n",
        "    print(\"Kendalls Tau is\",(Conc-Disc)/(Conc+Disc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-wWu6Ltc1f"
      },
      "source": [
        "### Interpretation :\n",
        "\n",
        "The Tau correlation coefficient returns a value of 0 to 1, where:\n",
        "\n",
        "0 is no relationship,\n",
        "1 is a perfect relationship\n",
        "\n",
        "It can also produce negative values (i.e. from -1 to 0). Unlike a linear graph, a negative relationship doesn’t mean much with ranked columns, so just remove the negative sign when you’re interpreting Tau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Wx2YzI6AFJ"
      },
      "source": [
        "# 21. Log loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dgnHGcn7GKL"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def lloss(y_test, y_pred):\n",
        "  Logloss=log_loss(y_test,y_pred)\n",
        "  print(\"Log Loss is\",Logloss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJj9yxO44Tgu"
      },
      "source": [
        "### Interpretation :\n",
        "\n",
        "Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYCrAUZq6Rkx"
      },
      "source": [
        "# 22. Brier score\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhyKK5Yq7G1P"
      },
      "source": [
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "def BSL(y_test,y_pred_prob):\n",
        "  BrierSL=brier_score_loss(y_test,y_pred_prob)\n",
        "  print(\"Brier Score Loss  is\",BrierSL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Paa1ETrPx5Wf"
      },
      "source": [
        "### Interpretation:\n",
        "\n",
        "Lower the Brier score is for a set of predictions, the better the predictions are calibrated.\n",
        "\n",
        "1. If the predicted probability is 1 and it happens, then the Brier Score is 0, the best score achievable.\n",
        "\n",
        "2. If the predicted probability is 1 and it does not happen, then the Brier Score is 1, the worst score achievable.\n",
        "\n",
        "3. If the predicted probability is 0.8 and it happens, then the Brier Score is (0.8-1)^2 =0.04.\n",
        "\n",
        "4. If the predicted probability is 0.2 and it happens, then the Brier Score is (0.2-1)^2 =0.64.\n",
        "\n",
        "5. If the predicted probability is 0.5, then the Brier Score is (0.5-1)^2 =0.25, irregardless of whether it happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx20_BMY6R-H"
      },
      "source": [
        "# 23. Gini Coefficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsDVWi9s7Igz"
      },
      "source": [
        "def gini(y_test, y_pred):\n",
        "    assert (len(y_test) == len(y_pred))\n",
        "    all = np.asarray(np.c_[y_test, y_pred, np.arange(len(y_test))], dtype=np.float)\n",
        "    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n",
        "    totalLosses = all[:, 0].sum()\n",
        "    giniSum = all[:, 0].cumsum().sum() / totalLosses\n",
        "\n",
        "    giniSum = (len(y_test) + 1) / 2\n",
        "    return giniSum / len(y_test)\n",
        "\n",
        "\n",
        "def gini_normalized(y_test, y_pred):\n",
        "    return gini(y_test, y_pred) / gini(y_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYJSp21irccg"
      },
      "source": [
        "### Interpretation :\n",
        "\n",
        "Gini Coefficient can take values between -1 and 1. Negative values correspond to a model with reversed meanings of scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpakAOHB6SIF"
      },
      "source": [
        "# 24. Kolmogorov Smirnov statistics and Kolmogorov-Smirnov plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ymv0GYr7JBp"
      },
      "source": [
        "def ks(data=None,target=None, prob=None):\n",
        "    data['target0'] = 1 - data[target]\n",
        "    data['bucket'] = pd.qcut(data[prob], 10)\n",
        "    grouped = data.groupby('bucket', as_index = False)\n",
        "    kstable = pd.DataFrame()\n",
        "    kstable['min_prob'] = grouped.min()[prob]\n",
        "    kstable['max_prob'] = grouped.max()[prob]\n",
        "    kstable['events']   = grouped.sum()[target]\n",
        "    kstable['nonevents'] = grouped.sum()['target0']\n",
        "    kstable = kstable.sort_values(by=\"min_prob\", ascending=False).reset_index(drop = True)\n",
        "    kstable['event_rate'] = (kstable.events / data[target].sum()).apply('{0:.2%}'.format)\n",
        "    kstable['nonevent_rate'] = (kstable.nonevents / data['target0'].sum()).apply('{0:.2%}'.format)\n",
        "    kstable['cum_eventrate']=(kstable.events / data[target].sum()).cumsum()\n",
        "    kstable['cum_noneventrate']=(kstable.nonevents / data['target0'].sum()).cumsum()\n",
        "    kstable['KS'] = np.round(kstable['cum_eventrate']-kstable['cum_noneventrate'], 3) * 100\n",
        "\n",
        "    #Formating\n",
        "    kstable['cum_eventrate']= kstable['cum_eventrate'].apply('{0:.2%}'.format)\n",
        "    kstable['cum_noneventrate']= kstable['cum_noneventrate'].apply('{0:.2%}'.format)\n",
        "    kstable.index = range(1,11)\n",
        "    kstable.index.rename('Decile', inplace=True)\n",
        "    pd.set_option('display.max_columns', 10)\n",
        "    print(kstable)\n",
        "    \n",
        "    #Display KS\n",
        "    from colorama import Fore\n",
        "    print(Fore.RED + \"KS is \" + str(max(kstable['KS']))+\"%\"+ \" at decile \" + str((kstable.index[kstable['KS']==max(kstable['KS'])][0])))\n",
        "    return(kstable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22FJDuQ9bwYV"
      },
      "source": [
        "### Interpretation :\n",
        "\n",
        "1. KS statistics should be in top 3 deciles.\n",
        "\n",
        "2. KS statistics should be between 40 and 70. Should not be significantly different from Training KS.\n",
        "\n",
        "3. Score above 70 is susceptible and might be overfitting so rigorous validation is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xc_yOkg9AGc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62hhjMhZcZwU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qaCBC5DcZ0A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n08FAt7cZ3F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8G2H7tGcZ5k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKy_2B8ucZ9H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwcGWB3ncaB9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5W2a-nkZwLi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}